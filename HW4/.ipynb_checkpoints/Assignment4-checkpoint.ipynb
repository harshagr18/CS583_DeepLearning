{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CERWeJIMPwGY"
   },
   "source": [
    "# Assignment 4: Build a Supervised Autoencoder.\n",
    "\n",
    "### Name: Harsh Agrawal\n",
    "\n",
    "### Due Date: Tuesday 5/3/2022 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5qGf_LLPwGZ"
   },
   "source": [
    "\n",
    "PCA and the standard autoencoder are unsupervised dimensionality reduction methods, and their learned features are not discriminative. If you build a classifier upon the low-dimenional features extracted by PCA and autoencoder, you will find the classification accuracy very poor.\n",
    "\n",
    "Linear discriminant analysis (LDA) is a traditionally supervised dimensionality reduction method for learning low-dimensional features which are highly discriminative. Likewise, can we extend autoencoder to supervised leanring?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIFKjnABPwGZ"
   },
   "source": [
    "**You are required to build and train a supervised autoencoder look like the following.** You are required to add other layers properly to alleviate overfitting.\n",
    "\n",
    "\n",
    "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNohSFbFPwGa"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Build a standard dense autoencoder, visual the low-dim features and the reconstructions, and evaluate whether the learned low-dim features are discriminative.\n",
    "\n",
    "2. Repeat the above process by training a supervised autoencoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziHiYnJwPwGa"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTcBJb-WPwGa"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mZ8T6_AxPwGa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Shape of x_train: (60000, 784)\n",
      "Shape of x_test: (10000, 784)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 28*28).astype('float32') / 255.\n",
    "x_test = x_test.reshape(10000, 28*28).astype('float32') / 255.\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rLwXo0yPwGb"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YyI16V84PwGb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (60000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_one_hot(y, num_class=10):\n",
    "    results = np.zeros((len(y), num_class))\n",
    "    for i, label in enumerate(y):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbfl7Um8PwGc"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 60K training samples to 2 sets:\n",
    "* a training set containing 10K samples;\n",
    "* a validation set containing 50K samples. (You can use only 10K to save time.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fnjlaZzuPwGc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (10000, 784)\n",
      "Shape of y_tr: (10000, 10)\n",
      "Shape of x_val: (10000, 784)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = np.random.permutation(60000)\n",
    "train_indices = rand_indices[0:10000]\n",
    "valid_indices = rand_indices[10000:20000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVjMusdlPwGc"
   },
   "source": [
    "## 2. Build an unsupervised  autoencoder and tune its hyper-parameters\n",
    "\n",
    "1. Build a dense autoencoder model\n",
    "2. Your encoder should contain 3 dense layers and 1 bottlenect layer with 2 as  output size. \n",
    "3. Your decoder should contain 4 dense layers with 784 as output size.\n",
    "4. You can choose different number of hidden units in dense layers.\n",
    "5. Do not add other layers (no activation layers), you may add them in later sections.\n",
    "6. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "    \n",
    "7. Try to achieve a validation loss as low as possible.\n",
    "8. Evaluate the model on the test set.\n",
    "9. Visualize the low-dim features and reconstructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbKlAcMFPwGc"
   },
   "source": [
    "### 2.1. Build the model (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dL5MH6UlPwGc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_img (InputLayer)       [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "encode1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "encode2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "encode3 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "bottleneck (Dense)           (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "decode1 (Dense)              (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "decode2 (Dense)              (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "decode3 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "decode4 (Dense)              (None, 784)               101136    \n",
      "=================================================================\n",
      "Total params: 210,562\n",
      "Trainable params: 210,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras import models\n",
    "\n",
    "input_img = Input(shape=(784,), name='input_img')\n",
    "\n",
    "encode1 = Dense(128, activation='relu', name='encode1')(input_img)\n",
    "encode2 = Dense(32, activation='relu', name='encode2')(encode1)\n",
    "encode3 = Dense(8, activation='relu', name='encode3')(encode2)\n",
    "bottleneck = Dense(2, activation='relu', name='bottleneck')(encode3)\n",
    "decode1 = Dense(8, activation='relu', name='decode1')(bottleneck)\n",
    "decode2 = Dense(32, activation='relu', name='decode2')(decode1)\n",
    "decode3 = Dense(128, activation='relu', name='decode3')(decode2)\n",
    "decode4 = Dense(784, activation='relu', name='decode4')(decode3)\n",
    "\n",
    "\n",
    "ae = models.Model(input_img, decode4)\n",
    "\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mq0XpDhTPwGd"
   },
   "outputs": [],
   "source": [
    "# print the network structure to a PDF file\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(ae, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=ae, show_shapes=False,\n",
    "    to_file='unsupervised_ae.pdf'\n",
    ")\n",
    "\n",
    "# you can find the file \"unsupervised_ae.pdf\" in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSf8i8QHPwGd"
   },
   "source": [
    "### 2.2. Train the model and tune the hyper-parameters (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "viuWz8tWPwGd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x000001B1AA04DF70>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-98c24eab7e33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1E-3\u001b[0m \u001b[1;31m# to be tuned!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m ae.compile(loss='mean_squared_error',\n\u001b[0m\u001b[0;32m      6\u001b[0m            optimizer=optimizers.RMSprop(lr=learning_rate))\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m       self.compiled_loss = compile_utils.LossesContainer(\n\u001b[0;32m    550\u001b[0m           loss, loss_weights, output_names=self.output_names)\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_get_optimizer\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    584\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_single_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_get_single_optimizer\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_single_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m       \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m       if (loss_scale is not None and\n\u001b[0;32m    579\u001b[0m           not isinstance(opt, lso.LossScaleOptimizer)):\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    133\u001b[0m         'Could not interpret optimizer identifier: {}'.format(identifier))\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x000001B1AA04DF70>"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "learning_rate = 1E-3 # to be tuned!\n",
    "\n",
    "ae.compile(loss='mean_squared_error',\n",
    "           optimizer=optimizers.RMSprop(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DeL_ZDzPwGd"
   },
   "outputs": [],
   "source": [
    "history = ae.fit(x_tr, x_tr, \n",
    "                 batch_size=128, \n",
    "                 epochs=100, \n",
    "                 validation_data=(x_val, x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTeRNc6ePwGe"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGCqJmkZPwGe"
   },
   "source": [
    "### 2.3. Visualize the reconstructed test images (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sk0TixTcPwGe"
   },
   "outputs": [],
   "source": [
    "ae_output = ae.predict(x_test).reshape((10000, 28, 28))\n",
    "\n",
    "ROW = 5\n",
    "COLUMN = 4\n",
    "\n",
    "x = ae_output\n",
    "fname = 'reconstruct_ae.pdf'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
    "for ax, i in zip(axes.flat, np.arange(ROW*COLUMN)):\n",
    "    image = x[i].reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC9oKRltPwGe"
   },
   "source": [
    "### 2.4. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viaZ0vGXPwGe"
   },
   "outputs": [],
   "source": [
    "loss = ae.evaluate(x_test, x_test)\n",
    "print('loss = ' + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiH0YYflPwGe"
   },
   "source": [
    "### 2.5. Visualize the low-dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjNJXDsIPwGf"
   },
   "outputs": [],
   "source": [
    "# build the encoder network\n",
    "ae_encoder = models.Model(input_img, bottleneck)\n",
    "ae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KufyF8uPwGf"
   },
   "outputs": [],
   "source": [
    "# extract low-dimensional features from the test data\n",
    "encoded_test = ae_encoder.predict(x_test)\n",
    "print('Shape of encoded_test: ' + str(encoded_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdA-flR5PwGf"
   },
   "outputs": [],
   "source": [
    "colors = np.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
    "colors_test = colors[y_test]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "fname = 'ae_code.pdf'\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr0UMMcAPwGf"
   },
   "source": [
    "#### Remark:\n",
    "\n",
    "Judging from the visualization, the low-dim features seems not discriminative, as 2D features from different classes are mixed. Let quantatively find out whether they are discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjRVaPmvPwGf"
   },
   "source": [
    "## 3. Are the learned low-dim features discriminative? (10 points)\n",
    "\n",
    "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPvkWFAvPwGf"
   },
   "outputs": [],
   "source": [
    "# extract the 2D features from the training, validation, and test samples\n",
    "f_tr = ae_encoder.predict(x_tr)\n",
    "f_val = ae_encoder.predict(x_val)\n",
    "f_te = ae_encoder.predict(x_test)\n",
    "\n",
    "print('Shape of f_tr: ' + str(f_tr.shape))\n",
    "print('Shape of f_te: ' + str(f_te.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsp4Gbr9PwGf"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras import models\n",
    "\n",
    "input_feat = Input(shape=(2,))\n",
    "\n",
    "hidden1 = Dense(128, activation='relu')(input_feat)\n",
    "hidden2 = Dense(128, activation='relu')(hidden1)\n",
    "output = Dense(10, activation='softmax')(hidden2)\n",
    "\n",
    "classifier = models.Model(input_feat, output)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qd2igsB6PwGg"
   },
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=1E-4),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = classifier.fit(f_tr, y_tr, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        validation_data=(f_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz_ewffqPwGg"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Using the 2D features, the validation accuracy is 60~70%. Recall that using the original data, the accuracy is about 97%. Obviously, the 2D features are not very discriminative.\n",
    "\n",
    "We are going to build a supervised autoencode model for learning low-dimensional discriminative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "by7uYcaTPwGg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQP18BwIPwGg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2nW56WAPwGg"
   },
   "source": [
    "## 4. Build a supervised autoencoder model\n",
    "\n",
    "\n",
    "**You are required to build and train a supervised autoencoder look like the following.** (Not necessary the same. You can use convolutional layers as well.) You are required to add other layers properly to alleviate overfitting.\n",
    "\n",
    "\n",
    "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec-KwvH9PwGg"
   },
   "source": [
    "### 4.1. Build the network (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "R7ZLYqHoPwGg"
   },
   "outputs": [],
   "source": [
    "# build the supervised autoencoder network\n",
    "from keras.layers import *\n",
    "from keras import models\n",
    "\n",
    "input_img = Input(shape=(784,), name='input_img')\n",
    "\n",
    "# encoder network\n",
    "\n",
    "encode1 = <add a dense layer taking input_img as input>\n",
    "<Add more layers...>\n",
    "\n",
    "# The width of the bottleneck layer must be exactly 2.\n",
    "\n",
    "bottleneck = <the output of encoder network>\n",
    "\n",
    "# decoder network\n",
    "decode1 =  <add a dense layer taking bottleneck as input>\n",
    "<Add more layers...>\n",
    "decode4 = <the output of decoder network>\n",
    "\n",
    "\n",
    "# build a classifier upon the bottleneck layer\n",
    "classifier1 = <add a dense layer taking bottleneck as input>\n",
    "<Add more dense layers and regularizations...>\n",
    "classifier3 = <the output of classifier network>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8TZHyOTPwGg"
   },
   "outputs": [],
   "source": [
    "# connect the input and the two outputs\n",
    "sae = models.Model(input_img, [decode4, classifier3])\n",
    "\n",
    "sae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6XBoTxgFPwGh"
   },
   "outputs": [],
   "source": [
    "# print the network structure to a PDF file\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(sae, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=sae, show_shapes=False,\n",
    "    to_file='supervised_ae.pdf'\n",
    ")\n",
    "\n",
    "# you can find the file \"supervised_ae.pdf\" in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHBa8fdtPwGh"
   },
   "source": [
    "### 4.2. Train the new model and tune the hyper-parameters\n",
    "\n",
    "The new model has multiple output. Thus we specify **multiple** loss functions and their weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pil1ijN0PwGh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "sae.compile(loss=['mean_squared_error', 'categorical_crossentropy'],\n",
    "            loss_weights=[1, 0.5], # to be tuned\n",
    "            optimizer=optimizers.RMSprop(lr=1E-3))\n",
    "\n",
    "history = sae.fit(x_tr, [x_tr, y_tr], \n",
    "                  batch_size=32, \n",
    "                  epochs=100, \n",
    "                  validation_data=(x_val, [x_val, y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrJHxmgVPwGh"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4IzyvuKPwGh"
   },
   "source": [
    "### Question  (10 points)\n",
    "\n",
    "Do you think overfitting is happening? If yes, what can you do? Please make necessary changes to the supervised autoencoder network structure.\n",
    "\n",
    "You can use the new model without overfitting for the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UgknLLGPwGh"
   },
   "source": [
    "### 4.3. Visualize the reconstructed test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQWjdq-sPwGh"
   },
   "outputs": [],
   "source": [
    "sae_output = sae.predict(x_test)[0].reshape((10000, 28, 28))\n",
    "\n",
    "ROW = 5\n",
    "COLUMN = 4\n",
    "\n",
    "x = sae_output\n",
    "fname = 'reconstruct_sae.pdf'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
    "for ax, i in zip(axes.flat, np.arange(ROW*COLUMN)):\n",
    "    image = x[i].reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFS6yFBnPwGi"
   },
   "source": [
    "### 4.4. Visualize the low-dimensional features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrQES8hFPwGi"
   },
   "outputs": [],
   "source": [
    "# build the encoder model\n",
    "sae_encoder = models.Model(input_img, bottleneck)\n",
    "sae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl77LBCIPwGi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract test features\n",
    "encoded_test = sae_encoder.predict(x_test)\n",
    "print('Shape of encoded_test: ' + str(encoded_test.shape))\n",
    "\n",
    "colors = np.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
    "colors_test = colors[y_test]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "fname = 'sae_code.pdf'\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sX7uwbePwGi"
   },
   "source": [
    "### 4.5. Are the learned low-dim features discriminative? (10 points)\n",
    "\n",
    "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the validation and test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qJpZmHXLPwGi"
   },
   "outputs": [],
   "source": [
    "# extract 2D features from the training, validation, and test samples\n",
    "f_tr = sae_encoder.predict(x_tr)\n",
    "f_val = sae_encoder.predict(x_val)\n",
    "f_te = sae_encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYeL97nrPwGi"
   },
   "outputs": [],
   "source": [
    "# build a classifier which takes the 2D features as input\n",
    "from keras.layers import *\n",
    "from keras import models\n",
    "\n",
    "input_feat = Input(shape=(2,))\n",
    "\n",
    "<build a classifier which takes input_feat as input>\n",
    "output = <output of the classifier network>\n",
    "\n",
    "classifier = models.Model(input_feat, output)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5oAu1pwPwGi"
   },
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=1E-4),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = classifier.fit(f_tr, y_tr, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        validation_data=(f_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF6gznU0PwGi"
   },
   "source": [
    "#### Remark: (10 points)\n",
    "\n",
    "The validation accuracy must be above 90%. It means the low-dim features learned by the supervised autoencoder are very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETJ_shAxPwGj"
   },
   "outputs": [],
   "source": [
    "# evaluate your model on the never-seen-before test data\n",
    "# write your code here:\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bonus2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
